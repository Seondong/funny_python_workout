{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow\n",
    "Blog: https://tensorflowkorea.wordpress.com/2016/05/24/fizz-buzz-in-tensorflow/\n",
    "Original Blog: http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/\n",
    "Code: https://github.com/joelgrus/fizz-buzz-tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fizz Buzz in Tensorflow!\n",
    "# see http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_DIGITS = 10\n",
    "\n",
    "# Represent each input by an array of its binary digits.\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "# One-hot encode the desired outputs: [number, \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return np.array([0, 0, 0, 1])\n",
    "    elif i % 5  == 0: return np.array([0, 0, 1, 0])\n",
    "    elif i % 3  == 0: return np.array([0, 1, 0, 0])\n",
    "    else:             return np.array([1, 0, 0, 0])\n",
    "\n",
    "# Our goal is to produce fizzbuzz for the numbers 1 to 100. So it would be\n",
    "# unfair to include these in our training data. Accordingly, the training data\n",
    "# corresponds to the numbers 101 to (2 ** NUM_DIGITS - 1).\n",
    "trX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "trY = np.array([fizz_buzz_encode(i)          for i in range(101, 2 ** NUM_DIGITS)])\n",
    "\n",
    "# We'll want to randomly initialize weights.\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "# Our model is a standard 1-hidden-layer multi-layer-perceptron with ReLU\n",
    "# activation. The softmax (which turns arbitrary real-valued outputs into\n",
    "# probabilities) gets applied in the cost function.\n",
    "def model(X, w_h, w_o):\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "    return tf.matmul(h, w_o)\n",
    "\n",
    "# Our variables. The input has width NUM_DIGITS, and the output has width 4.\n",
    "X = tf.placeholder(\"float\", [None, NUM_DIGITS])\n",
    "Y = tf.placeholder(\"float\", [None, 4])\n",
    "\n",
    "# How many units in the hidden layer.\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "# Initialize the weights.\n",
    "w_h = init_weights([NUM_DIGITS, NUM_HIDDEN])\n",
    "w_o = init_weights([NUM_HIDDEN, 4])\n",
    "\n",
    "# Predict y given x using the model.\n",
    "py_x = model(X, w_h, w_o)\n",
    "\n",
    "# We'll train our model by minimizing a cost function.\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n",
    "\n",
    "# And we'll make predictions by choosing the largest output.\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "# Finally, we need a way to turn a prediction (and an original number)\n",
    "# into a fizz buzz output\n",
    "def fizz_buzz(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.383531960997\n",
      "200 0.534127843987\n",
      "400 0.534127843987\n",
      "600 0.534127843987\n",
      "800 0.614301191766\n",
      "1000 0.635969664139\n",
      "1200 0.687973997833\n",
      "1400 0.796316359697\n",
      "1600 0.867822318527\n",
      "1800 0.890574214518\n",
      "2000 0.912242686891\n",
      "2200 0.926327193933\n",
      "2400 0.886240520043\n",
      "2600 0.94474539545\n",
      "2800 0.955579631636\n",
      "3000 0.958829902492\n",
      "3200 0.959913326111\n",
      "3400 0.964247020585\n",
      "3600 0.950162513543\n",
      "3800 0.979414951246\n",
      "4000 0.967497291441\n",
      "4200 0.975081256771\n",
      "4400 0.981581798483\n",
      "4600 0.98374864572\n",
      "4800 0.985915492958\n",
      "5000 0.986998916576\n",
      "5200 0.986998916576\n",
      "5400 0.986998916576\n",
      "5600 0.958829902492\n",
      "5800 0.99241603467\n",
      "6000 0.991332611051\n",
      "6200 0.994582881907\n",
      "6400 0.995666305525\n",
      "6600 0.995666305525\n",
      "6800 0.997833152763\n",
      "7000 0.99241603467\n",
      "7200 0.997833152763\n",
      "7400 0.996749729144\n",
      "7600 0.997833152763\n",
      "7800 0.998916576381\n",
      "8000 0.998916576381\n",
      "8200 0.998916576381\n",
      "8400 1.0\n",
      "8600 1.0\n",
      "8800 1.0\n",
      "9000 1.0\n",
      "9200 0.998916576381\n",
      "9400 1.0\n",
      "9600 1.0\n",
      "9800 1.0\n",
      "['1' '2' 'fizz' '4' 'buzz' 'fizz' '7' '8' 'fizz' 'buzz' '11' 'fizz' '13'\n",
      " '14' 'fizzbuzz' '16' '17' 'fizz' '19' 'buzz' 'fizz' '22' '23' 'fizz'\n",
      " 'buzz' '26' 'fizz' '28' '29' 'fizzbuzz' '31' '32' 'fizz' '34' 'buzz'\n",
      " 'fizz' '37' '38' 'fizz' 'buzz' '41' 'fizz' '43' '44' 'fizzbuzz' '46' '47'\n",
      " 'fizz' '49' 'buzz' 'fizz' '52' '53' 'fizz' 'buzz' '56' 'fizz' '58' '59'\n",
      " 'fizzbuzz' '61' '62' 'fizz' '64' 'buzz' 'buzz' '67' '68' 'buzz' 'buzz'\n",
      " '71' '72' '73' '74' 'fizzbuzz' '76' '77' 'fizz' 'buzz' 'buzz' 'fizz' '82'\n",
      " '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' 'fizzbuzz' '91' '92' 'fizz' '94'\n",
      " 'buzz' 'fizz' '97' '98' 'fizz' '100']\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for epoch in range(10000):\n",
    "        # Shuffle the data before each training iteration.\n",
    "        p = np.random.permutation(range(len(trX)))\n",
    "        trX, trY = trX[p], trY[p]\n",
    "\n",
    "        # Train in batches of 128 inputs.\n",
    "        for start in range(0, len(trX), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "\n",
    "        # And print the current accuracy on the training data.\n",
    "        if epoch % 200== 0:\n",
    "            print(epoch, np.mean(np.argmax(trY, axis=1) == sess.run(predict_op, feed_dict={X: trX, Y: trY})))\n",
    "        \n",
    "    # And now for some fizz buzz\n",
    "    numbers = np.arange(1, 101)\n",
    "    teX = np.transpose(binary_encode(numbers, NUM_DIGITS))\n",
    "    teY = sess.run(predict_op, feed_dict={X: teX})\n",
    "    output = np.vectorize(fizz_buzz)(numbers, teY)\n",
    "\n",
    "    print(output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[2,3,4],[1,2,5]]    \n",
    "np.argmax(a, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
